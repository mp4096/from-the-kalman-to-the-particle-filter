<!doctype html>
<html lang="en">

<!--
Copyright (C) 2016 Mikhail Pak
CC-BY-NC-SA 4.0

This slide deck uses reveal.js by Hakim El Hattab.
Copyright (C) 2016 Hakim El Hattab, http://hakim.se
MIT License
-->

<head>
	<meta charset="utf-8">

	<link href="https://fonts.googleapis.com/css?family=Open+Sans:200|Raleway:200|Source+Code+Pro" rel="stylesheet">

	<title>From the Kalman to the particle filter</title>

	<meta name="description" content="From the Kalman to the particle filter">
	<meta name="author" content="Mikhail Pak, TUM">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/winter.css" id="theme">

	<link rel="stylesheet" href="lib/css/color-brewer.css">
</head>

<body>
<div class="reveal">
<div class="slides">

<section class="center">
	<h2>From the Kalman to the particle filter</h2>
	<small>
		<p><a style="color:#222" href="https://github.com/mp4096">Mikhail Pak</a>, November 2016</p>
		<p style="color:#EEE">CC-BY-NC-SA 4.0</p>
	</small>
</section>

<section>
	<h3>Outline</h3>
	<ul>
		<li><p><a style="color:#222" href="#/2">Introduction</a></p></li>
		<li><p><a style="color:#222" href="#/5">Kalman filter as a Bayesian estimator</a></p></li>
		<li><p><a style="color:#222" href="#/6">Sequential importance sampling</a></p></li>
		<li><p><a style="color:#222" href="#/10">Making it work: SIR</a></p></li>
		<!-- TODO: Slide numbers -->
		<li><p><a style="color:#222" href="#/1">Further extensions of the particle filter</a></p></li>
		<li><p><a style="color:#222" href="#/1">Comparison to EKF, UKF and EnKF</a></p></li>
		<li><p><a style="color:#222" href="#/1">An example: Face tracking</a></p></li>
		<li><p><a style="color:#222" href="#/1">Conclusion</a></p></li>
	</ul>
</section>

<section>
	<h3>Introduction</h3>
	<p>
		The Kalman filter is well-known in the control engineering community.
		Its extensions (EKF, UKF, EnKF, mixture KF) allow handling of nonlinear, non-Gaussian problems.
	</p>
	<p class="fragment">
		On the other hand, we can also tackle the nonlinear filtering problem using a <em>particle filter</em>.
		In this talk, I will<ul>
			<li class="fragment"><p>show how both the Kalman and the particle filter can be derived
				using the same Bayesian framework;</p></li>
			<li class="fragment"><p>present the most widespread variants of the particle filter;</p></li>
			<li class="fragment"><p>discuss the design choices for a particle filter;</p></li>
			<li class="fragment"><p>compare it with the extensions of the Kalman filter;</p></li>
			<li class="fragment"><p>show a simple demo of a particle filter for face tracking.</p></li>
		</ul>
	</p>
</section>

<section>
	<h3>Introduction</h3>
	<p>
		We consider a discrete-time autonomous system.
		Furthermore, we assume that it satisfies the Markov property.
	</p>
	<p class="fragment">
		For a time instant $k$, we denote<ul>
			<li class="fragment"><p>state by $x_k \in \mathbb{R}^n$;</p></li>
			<li class="fragment"><p>state trajectory $x_0, \ldots, x_k$ by $\mathcal{X}_k$;</p></li>
			<li class="fragment"><p>measurement by $y_k \in \mathbb{R}^q$;</p></li>
			<li class="fragment"><p>sequence of measurements $y_0, \ldots, y_k$ by $\mathcal{Y}_k$;</p></li>
			<li class="fragment"><p>transition pdf by $p(x_k | x_{k - 1})$;</p></li>
			<li class="fragment"><p>measurement pdf by $p(y_k | x_k)$.</p></li>
		</ul>
</section>

<section>
	<h3>Introduction</h3>
	<p>Now, let’s revise Bayesian inference before we discuss the filtering problem.</p>
	<p>All derivations will be based on Bayes’ theorem:</p>
	<script type="math/tex; mode=display">
		p(A \,|\,B) = \frac{p(B \,|\,A) \ p(A)}{p(B)}
	</script>
	<div class="fragment">
	or, informally:
	<script type="math/tex; mode=display">
		p(\text{State} \,|\,\text{Measurements} )
		=
		\frac{
			p(\text{Measurements} \,|\,\text{State} )\ p(\text{State})
		}{
			p(\text{Measurements})
		}.
	</script></p>
	</div>
	<div class="fragment">
	<p>Here,<ul>
		<li class="fragment"><p>$p(\text{State}\,|\,\text{Measurements})$ is the <em>posterior</em>;</p></li>
		<li class="fragment"><p>$p(\text{Measurements}\,|\,\text{State})$ is the <em>likelihood</em>;</p></li>
		<li class="fragment"><p>$p(\text{State})$ is the <em>prior</em>;</p></li>
		<li class="fragment"><p>$p(\text{Measurements})$ is the <em>total likelihood</em>.</p></li>
	</ul></p>
	</div>
</section>

<section>
	<h3>Kalman filter as a Bayesian estimator</h3>
	<p>
		We can derive the Kalman filter directly by plugging Gaussian pdfs into the Bayes formula:
	</p>
	<p class="fragment">
	<script type="math/tex; mode=display">
		p(x_k \,|\,y_k, \mathcal{Y}_{k - 1})
		=
		\frac{%
			p(y_k \,|\,x_k, \mathcal{Y}_{k - 1})
			\ %
			p(x_k \,|\,\mathcal{Y}_{k - 1})
		}{%
			p(y_k \,|\,\mathcal{Y}_{k - 1})
		}.
	</script>
	</p>
	<p style="text-align:center" class="fragment">
		$\downarrow$<br>
		<script type="math/tex; mode=display">
			\left. \mathcal{N}(\hat{x}_k, \ P_k) \right|_{x_k}
			=
			\frac{%
				\left. \mathcal{N}(C_k x_k, \ R_k) \right|_{y_k}
				\ %
				\left. \mathcal{N}(\hat{x}_k^-, \ P_k^-) \right|_{x_k}
			}{%
				\left. \mathcal{N}(\hat{y}_k^-, \ C_k P_k^- C_k^\mathrm{T} + R_k) \right|_{y_k}
			}.
		</script><br>
	</p>
	<p class="fragment">
		After some math, we obtain:
		<script type="math/tex; mode=display">
			\begin{aligned}
				\hat{x}_k &= \hat{x}_k^- + P_k C_k^\mathrm{T} R_k^{-1} (y_k - \hat{y}_k^-), \\
				P_k  &= \left( (P_k^-)^{-1} + C_k^\mathrm{T} R_k^{-1} C_k \right)^{-1}.
				% = \left( I - P_k^- C_k^\mathrm{T} (C_k P_k^- C_k^\mathrm{T} + R_k)^{-1} C_k \right) P_k^-.
			\end{aligned}
		</script>
	</p>
</section>

<section>
	<h3>Sequential importance sampling 1/4</h3>
	<p>
		The main idea of a particle filter is to approximate the posterior pdf
		by a finite number of samples (particles):
	</p>
	<script type="math/tex; mode=display">
		p(\mathcal{X}_k | \mathcal{Y}_k) \approx \sum_{i = 1}^{N} \delta(\mathcal{X}_k - \mathcal{X}^{(i)}_k),
	</script>
	<p class="fragment">
		where $\delta$ is the Dirac measure and $\mathcal{X}^{(i)}_k$ is the $i$-th sample drawn from $p(\mathcal{X}_k | \mathcal{Y}_k)$.
	</p>
	<p class="fragment">
		Unfortunately, this naïve approach has two significant disadvantages:
		<ol>
			<li class="fragment"><p>
				it converges very slowly, since we sample from the whole state space,
				including its &lsquo;unimportant&rsquo; regions.
			</p></li>
			<li class="fragment"><p>
				the posterior pdf itself is unknown; furthermore, it can be very difficult to draw samples
				from this distribution.
			</p></li>
		</ol>
	</p>
</section>

<section>
	<h3>Sequential importance sampling 2/4</h3>
	<p>
		Enter <em>importance sampling</em>.
	</p>
	<p>
		We can extend a probability distribution $p(x)$ by
		$q(x) q(x)^{-1} = 1$ without changing it.
		Hence, we can approximate the posterior pdf
		by a finite number of <em>weighted</em> samples:
	</p>
	<script type="math/tex; mode=display">
		p(\mathcal{X}_k | \mathcal{Y}_k) \approx \sum_{i = 1}^{N} w^{(i)}_k \delta(\mathcal{X}_k - \mathcal{X}^{(i)}_k),
	</script>
	<p class="fragment">
		where $\mathcal{X}^{(i)}_k$ are drawn from the <em>proposal pdf</em> $q(\mathcal{X}_k | \mathcal{Y}_k)$ and
		the weights are chosen as<br><br>
		<script type="math/tex; mode=display">
			w^{(i)}_k \propto \frac{p(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)}{q(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)}.
		</script>
	</p>
	<p class="fragment">
		Obviously, we would try to choose $q(\mathcal{X}_k | \mathcal{Y}_k)$ such that it is easy to sample.
		Furthermore, we restrict it to the following form:
	</p>
	<p class="fragment"><script type="math/tex; mode=display">
		q(\mathcal{X}_k | \mathcal{Y}_k) =
			q(x_k | \mathcal{X}_{k - 1}, \mathcal{Y}_k) \, q(\mathcal{X}_{k - 1} | \mathcal{Y}_{k - 1}).
	</script></p>
</section>

<section>
	<h3>Sequential importance sampling 3/4</h3>
	<p>
		Now we have to derive the weight update equation. Apply Bayes' theorem:
	</p>
	<script type="math/tex; mode=display">
		\begin{aligned}
			p(\mathcal{X}_k | \mathcal{Y}_k) &=
			\frac{
				p(y_k | \mathcal{X}_k, \mathcal{Y}_{k - 1}) \, p(\mathcal{X}_k | \mathcal{Y}_{k - 1})
			}{p(y_k | \mathcal{Y}_{k - 1})} \\[0.5 em]
			&\propto
				p(y_k | x_k) \, p(x_k | x_{k - 1}) p(\mathcal{X}_{k - 1} | \mathcal{Y}_{k - 1})
		\end{aligned}
	</script>
	<p class="fragment">
		and thus<br>
		<script type="math/tex; mode=display">
			\begin{aligned}
				w^{(i)}_k &\propto
				\frac{p(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)}{q(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)} \\[0.5 em]
				&\propto
				\frac{
						p(y_k | x^{(i)}_k) \, p(x^{(i)}_k | x^{(i)}_{k - 1}) p(\mathcal{X}^{(i)}_{k - 1} | \mathcal{Y}_{k - 1})
				}{q(x^{(i)}_k | \mathcal{X}^{(i)}_{k - 1}, \mathcal{Y}_k) \, q(\mathcal{X}^{(i)}_{k - 1} | \mathcal{Y}_{k - 1})} \\[0.5 em]
 				&\propto
 				\frac{p(y_k | x^{(i)}_k) \, p(x^{(i)}_k | x^{(i)}_{k - 1})}{q(x^{(i)}_k | \mathcal{X}^{(i)}_{k - 1}, \mathcal{Y}_k)}
				w^{(i)}_{k - 1}.
			\end{aligned}
		</script>
	</p>
</section>

<section>
	<h3>Sequential importance sampling 4/4</h3>
	<p>
		We can also restrict the proposal density to $q(x_k | x_{k - 1}, y_k)$. Then,
	</p><br>
	<script type="math/tex; mode=display">
		w^{(i)}_k \propto
		\frac{p(y_k | x^{(i)}_k) \, p(x^{(i)}_k | x^{(i)}_{k - 1})}{q(x^{(i)}_k | x^{(i)}_{k - 1}, y_k)}
		w^{(i)}_{k - 1}
	</script><br>
	<p class="fragment">
		and we can approximate the most recent state pdf as<br><br>
		<script type="math/tex; mode=display">
			p(x_k | \mathcal{Y}_k) \approx \sum_{i = 1}^{N} w^{(i)}_k \delta(x_k - x^{(i)}_k),
		</script>
	</p><br>
	<p class="fragment">
		We can summarise this algorithm in two steps: 1) draw samples from the proposal pdf;
		2) update the weigths. This algorithm is called <em>sequential importance sampling (SIS)</em>.
	</p>
</section>

<section>
	<h3>Making it work: SIR 1/TODO</h3>
	<p class="fragment">
		Theoretically, we could set all weights equal and draw $x^{(i)}_k$ directly from $p(x_k | \mathcal{Y}_k)$.
		However, this would be very ineffective, since we would sample from the whole state space,
		including its 	&lsquo;unimportant&rsquo; regions.
	</p>
	<p>
		Use the transition conditional distribution as the importance distribution function. -> SIS
	</p>
	<p>
		SIS suffers from the so called samples degeneracy, i.e. there are a lot of particles with
		very low weight while a couple of particles have huge weights and dominate the pdf.
	</p>
	<p>
		Solution: Resample (with replacemnt) if the number of effective particles falls below a certain
		threshold. -> SIR
	</p>
	<p>
		However, this, in turn, can lead to other problems such as samples impoverishment, i.e.
		the samples cluster all around the same point in state space.
		This happens especially often if the process noise is very small.
	</p>
	<p>
		Solution: Regularised particle filter.
	</p>
</section>

<section>
	<h3>Resampling methods</h3>
	<p>
		There are multiple resampling methods, all of which have some advantages and disadvantages:
		monomial, systematic, stratified and residual
	</p>
	<p>
		b
	</p>
</section>

<section>
	<h3>Comparison to EKF, UKF and EnKF</h3>
	<p>
		EKF/UKF/EnKF: Compute the exact solution of a simpler, approximated problem
	</p>
	<p>
		Particle filter: Compute the approximate solution of the exact problem
	</p>
	<p>
		UKF: Deterministic sampling, statistical linearisation to a unimodal Gaussian
		EnKF: Random sampling, still statistical linearisation to a unimodal Gaussian
	</p>
	<p>
		Limitations of the *KF: Cannot approximate multimodal pdfs, except for mixture Kalman filters
	</p>
</section>

<section>
	<h3>An example: Face tracking</h3>
	<p>
		a
	</p>
	<p>
		b
	</p>
</section>

<section>
	<h3>References</h3>
	<p>
		Thrun, Murphy, Bishop
	</p>
	<p>
		b
	</p>
</section>


</div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
	Reveal.initialize({
		controls: false,
		progress: true,
		history: true,
		center: false,

		transition: 'fade',

		math: {
			// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
			config: 'TeX-AMS_HTML-full',
		},

		dependencies: [
			{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
			{ src: 'plugin/math/math.js', async: true },
			{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.configure({ classPrefix: '' }); hljs.initHighlightingOnLoad(); } },
		],
	});
	Reveal.configure({
		slideNumber: true,
		slideNumber: 'c/t',
	});
</script>
</body>
</html>
