<!doctype html>
<html lang="en">

<!--
Copyright (C) 2016 Mikhail Pak
CC-BY-NC-SA 4.0

This slide deck uses reveal.js by Hakim El Hattab.
Copyright (C) 2016 Hakim El Hattab, http://hakim.se
MIT License
-->

<head>
	<meta charset="utf-8">

	<link href="https://fonts.googleapis.com/css?family=Open+Sans:200%7CRaleway:200%7CSource+Code+Pro" rel="stylesheet">

	<title>From the Kalman to the particle filter</title>

	<meta name="description" content="From the Kalman to the particle filter">
	<meta name="author" content="Mikhail Pak, TUM">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/winter.css" id="theme">

	<link rel="stylesheet" href="lib/css/color-brewer.css">

	<style>
		.github-corner:hover
		.octo-arm{animation:octocat-wave 560ms ease-in-out}
		@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}
		@media (max-width:500px) {
			.github-corner:hover
			.octo-arm{animation:none}
			.github-corner
			.octo-arm{animation:octocat-wave 560ms ease-in-out}
		}
	</style>
</head>


<body>
<a href="https://github.com/mp4096/from-the-kalman-to-the-particle-filter" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#AAA; color:#fff; z-index: 10; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>
<div class="reveal">
<div class="slides">

<section class="center">
	<h2>From the Kalman to the particle filter</h2>
	<p style="font-size: 75%"><a style="color:#222" href="https://github.com/mp4096">Mikhail Pak</a>, November 2016</p>
	<p style="color:#AAA; font-size: 75%">CC BY-NC-SA 4.0</p>
</section>

<section>
	<h3>Outline</h3>
	<ul>
		<li><p><a style="color:#222" href="#/2">Introduction</a></p></li>
		<li><p><a style="color:#222" href="#/4">Kalman filter as a Bayesian estimator</a></p></li>
		<li><p><a style="color:#222" href="#/5">Sequential importance sampling</a></p></li>
		<li><p><a style="color:#222" href="#/9">Making it work: SIR and RPF</a></p></li>
		<li><p><a style="color:#222" href="#/12">Comparison to the Kalman filter family</a></p></li>
		<li><p><a style="color:#222" href="#/13">An example: Robot localisation</a></p></li>
		<li><p><a style="color:#222" href="#/15">Conclusion</a></p></li>
	</ul>
</section>

<section>
	<h3>Introduction 1/2</h3>
	<p>
		The Kalman filter is well-known in the control engineering community.
		Its extensions (EKF, UKF, EnKF, mixture KF) allow handling of nonlinear, non-Gaussian problems.
	</p>
	<p class="fragment">
		<em>Particle filtering</em> is an alternative approach to the nonlinear filtering problem.
	</p><br>
	<p class="fragment">
		Consider a discrete-time Markovian autonomous system.
	</p>
	<div class="fragment">
		<p>For a time instant $k$, we denote</p>
		<ul>
			<li class="fragment"><p>state by $x_k$ and state trajectory $x_0, \ldots, x_k$ by $\mathcal{X}_k$;</p></li>
			<li class="fragment"><p>measurement by $y_k$ and sequence of measurements $y_0, \ldots, y_k$ by $\mathcal{Y}_k$;</p></li>
			<li class="fragment"><p>transition pdf by $p(x_k | x_{k - 1})$;</p></li>
			<li class="fragment"><p>measurement pdf by $p(y_k | x_k)$.</p></li>
		</ul>
	</div>
</section>

<section>
	<h3>Introduction 2/2</h3>
	<p>Now, let’s revise Bayesian inference before we discuss the filtering problem.</p>
	<p>All derivations will be based on Bayes’ theorem:</p>
	<script type="math/tex; mode=display">
		p(A \,|\,B) = \frac{p(B \,|\,A) \ p(A)}{p(B)}
	</script>
	<div class="fragment">
	or, informally:
	<script type="math/tex; mode=display">
		p(\text{State} \,|\,\text{Measurements} )
		=
		\frac{
			p(\text{Measurements} \,|\,\text{State} )\ p(\text{State})
		}{
			p(\text{Measurements})
		}.
	</script>
	</div>
	<div class="fragment">
	<p>Here,</p>
	<ul>
		<li class="fragment"><p>$p(\text{State}\,|\,\text{Measurements})$ is the <em>posterior</em>;</p></li>
		<li class="fragment"><p>$p(\text{Measurements}\,|\,\text{State})$ is the <em>likelihood</em>;</p></li>
		<li class="fragment"><p>$p(\text{State})$ is the <em>prior</em>;</p></li>
		<li class="fragment"><p>$p(\text{Measurements})$ is the <em>total likelihood</em>.</p></li>
	</ul>
	</div>
</section>

<section>
	<h3>Kalman filter as a Bayesian estimator</h3>
	<p>
		We can derive the Kalman filter directly by plugging Gaussian pdfs into the Bayes formula:
	</p>
	<div class="fragment">
		<script type="math/tex; mode=display">
			p(x_k \,|\,y_k, \mathcal{Y}_{k - 1})
			=
			\frac{%
				p(y_k \,|\,x_k, \mathcal{Y}_{k - 1})
				\ %
				p(x_k \,|\,\mathcal{Y}_{k - 1})
			}{%
				p(y_k \,|\,\mathcal{Y}_{k - 1})
			}.
		</script>
	</div>
	<div style="text-align:center" class="fragment">
		<p>$\downarrow$</p>
		<script type="math/tex; mode=display">
			\left. \mathcal{N}(\hat{x}_k, \ P_k) \right|_{x_k}
			=
			\frac{%
				\left. \mathcal{N}(C_k x_k, \ R_k) \right|_{y_k}
				\ %
				\left. \mathcal{N}(\hat{x}_k^-, \ P_k^-) \right|_{x_k}
			}{%
				\left. \mathcal{N}(\hat{y}_k^-, \ C_k P_k^- C_k^\mathrm{T} + R_k) \right|_{y_k}
			}.
		</script><br>
	</div>
	<div class="fragment">
		<p>After some math, we obtain:</p>
		<script type="math/tex; mode=display">
			\begin{aligned}
				\hat{x}_k &= \hat{x}_k^- + P_k C_k^\mathrm{T} R_k^{-1} (y_k - \hat{y}_k^-), \\
				P_k  &= \left( (P_k^-)^{-1} + C_k^\mathrm{T} R_k^{-1} C_k \right)^{-1}.
				% = \left( I - P_k^- C_k^\mathrm{T} (C_k P_k^- C_k^\mathrm{T} + R_k)^{-1} C_k \right) P_k^-,
			\end{aligned}
		</script>
		<p>
			with $\hat{x}_k^- = A_{k - 1} \hat{x}_{k - 1}$, $\hat{y}_k^- = C_k \hat{x}_k^-$ and
			$P_k^- = A_{k - 1} P_{k - 1} A_{k - 1}^\mathrm{T} + Q_k$.
		</p>
	</div>
</section>

<section>
	<h3>Sequential importance sampling 1/4</h3>
	<p>
		The main idea of a particle filter is to approximate the posterior pdf
		by a finite number of samples (particles):
	</p>
	<script type="math/tex; mode=display">
		p(\mathcal{X}_k | \mathcal{Y}_k) \approx \sum_{i = 1}^{N} \delta(\mathcal{X}_k - \mathcal{X}^{(i)}_k),
	</script>
	<p class="fragment">
		where $\delta$ is the Dirac measure and $\mathcal{X}^{(i)}_k$ is the $i$-th sample drawn from $p(\mathcal{X}_k | \mathcal{Y}_k)$.
	</p><br>
	<div class="fragment">
		<p>Unfortunately, this naïve approach has two significant disadvantages:</p>
		<ol>
			<li class="fragment"><p>
				it converges very slowly, since we sample from the whole state space,
				including its &lsquo;unimportant&rsquo; regions.
			</p></li>
			<li class="fragment"><p>
				the posterior pdf itself is unknown; furthermore, it can be very difficult to draw samples
				from this distribution.
			</p></li>
		</ol>
	</div>
</section>

<section>
	<h3>Sequential importance sampling 2/4</h3>
	<p>
		Enter <em>importance sampling</em>.
	</p>
	<p>
		We can extend a probability distribution $p(x)$ by
		$q(x) q(x)^{-1} = 1$ without changing it.
		Hence, we can approximate the posterior pdf
		by a finite number of <em>weighted</em> samples:
	</p>
	<script type="math/tex; mode=display">
		p(\mathcal{X}_k | \mathcal{Y}_k) \approx \sum_{i = 1}^{N} w^{(i)}_k \delta(\mathcal{X}_k - \mathcal{X}^{(i)}_k),
	</script>
	<div class="fragment">
		<p>
			where $\mathcal{X}^{(i)}_k$ are drawn from the <em>proposal pdf</em> $q(\mathcal{X}_k | \mathcal{Y}_k)$
			and the weights are chosen as
		</p>
		<script type="math/tex; mode=display">
			w^{(i)}_k \propto \frac{p(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)}{q(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)}.
		</script><br>
	</div>
	<div class="fragment">
		<p>
			Obviously, we would try to choose $q(\mathcal{X}_k | \mathcal{Y}_k)$ such that it is easy to sample.
			Furthermore, we restrict it to the following form:
		</p>
		<script type="math/tex; mode=display">
			q(\mathcal{X}_k | \mathcal{Y}_k) =
				q(x_k | \mathcal{X}_{k - 1}, \mathcal{Y}_k) \, q(\mathcal{X}_{k - 1} | \mathcal{Y}_{k - 1}).
		</script>
	</div>
</section>

<section>
	<h3>Sequential importance sampling 3/4</h3>
	<p>
		Now we have to derive the weight update equation. Apply Bayes' theorem:
	</p>
	<script type="math/tex; mode=display">
		\begin{aligned}
			p(\mathcal{X}_k | \mathcal{Y}_k) &=
			\frac{
				p(y_k | \mathcal{X}_k, \mathcal{Y}_{k - 1}) \, p(\mathcal{X}_k | \mathcal{Y}_{k - 1})
			}{p(y_k | \mathcal{Y}_{k - 1})} \\[0.5 em]
			&\propto
				p(y_k | x_k) \, p(x_k | x_{k - 1}) p(\mathcal{X}_{k - 1} | \mathcal{Y}_{k - 1})
		\end{aligned}
	</script>
	<div class="fragment">
		<p>and thus</p>
		<script type="math/tex; mode=display">
			\begin{aligned}
				w^{(i)}_k &\propto
				\frac{p(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)}{q(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)} \\[0.5 em]
				&\propto
				\frac{
						p(y_k | x^{(i)}_k) \, p(x^{(i)}_k | x^{(i)}_{k - 1}) p(\mathcal{X}^{(i)}_{k - 1} | \mathcal{Y}_{k - 1})
				}{q(x^{(i)}_k | \mathcal{X}^{(i)}_{k - 1}, \mathcal{Y}_k) \, q(\mathcal{X}^{(i)}_{k - 1} | \mathcal{Y}_{k - 1})} \\[0.5 em]
 				&\propto
 				\frac{p(y_k | x^{(i)}_k) \, p(x^{(i)}_k | x^{(i)}_{k - 1})}{q(x^{(i)}_k | \mathcal{X}^{(i)}_{k - 1}, \mathcal{Y}_k)}
				w^{(i)}_{k - 1}.
			\end{aligned}
		</script>
	</div>
</section>

<section>
	<h3>Sequential importance sampling 4/4</h3>
	<p>
		We can also restrict the proposal density to $q(x_k | x_{k - 1}, y_k)$. Then,
	</p><br>
	<script type="math/tex; mode=display">
		w^{(i)}_k \propto
		\frac{p(y_k | x^{(i)}_k) \, p(x^{(i)}_k | x^{(i)}_{k - 1})}{q(x^{(i)}_k | x^{(i)}_{k - 1}, y_k)}
		w^{(i)}_{k - 1}
	</script><br>
	<div class="fragment">
		<p>and we can approximate the most recent state pdf as</p>
		<script type="math/tex; mode=display">
			p(x_k | \mathcal{Y}_k) \approx \sum_{i = 1}^{N} w^{(i)}_k \delta(x_k - x^{(i)}_k),
		</script><br>
	</div>
	<p class="fragment">
		We can summarise this algorithm as follows: 1) Draw samples from the proposal pdf;
		2) Update the weigths. This is the so called <em>sequential importance sampling (SIS)</em>.
	</p>
</section>

<section>
	<h3>Making it work: SIR and RPF 1/3</h3>
	<p>
		The plain SIS algorithm suffers from the <em>degeneracy phenomenon</em>, i.e. the variance
		of the weights increases over time. Furthermore, we still have to choose an appropriate
		proposal pdf $q(x_k | x_{k - 1}, y_k)$.
	</p>
	<p class="fragment">
		The <em>sequential importance resampling (SIR)</em> algorithm is the most widespread particle filtering algorithm.
		Its first main &lsquo;ingredient&rsquo; is to resample the particles after each step to avoid degeneracy.
		There are many resampling methods, e.g. multinomial, residual, systematic and stratified.
		Due to resampling, $w^{(i)}_{k - 1} = N^{-1}$ $\forall i$.
	</p>
	<div class="fragment">
		<p>The second &lsquo;ingredient&rsquo; is to use the prior density as a proposal pdf</p>
		<script type="math/tex; mode=display">
			q(x_k | x^{(i)}_{k - 1}, y_k) = p(x_k | x^{(i)}_{k - 1})
		</script>
		<p>which, together with resampling, leads us to a very simple weight update equation:</p>
		<script type="math/tex; mode=display">
			w^{(i)}_k \propto p(y_k | x^{(i)}_k).
		</script>
	</div>
</section>

<section>
	<h3>Making it work: SIR and RPF 2/3</h3>
	<p>Thus, we can finally specify an algorithm for a usable particle filter:</p>
<pre><code class="lang-python" data-trim data-noescape>
for y in yield_measurements():

    # Propagate particles and update their weights
    x_curr = sample_from_prior(x_prev)
    w_curr = update_weight(y, x_curr)

    # Normalise weights
    w_curr /= sum(w_curr)

    # Resample particles according to their weights
    x_prev = resample(x_curr, w_curr)
</code></pre><br>
</section>

<section>
	<h3>Making it work: SIR and RPF 3/3</h3>
	<p>
		Unfortunately, SIR itself suffers from <em>sample impoverishment</em>,
		especially if the process noise is small.
		In this case, a <em>regularised particle filter (RPF)</em> might be a good solution.
	</p>
	<div class="fragment">
		<p>
			A regularised particle filter is identical to the SIR except that it resamples from
			a continuous approximation for the posterior pdf:
		</p>
		<script type="math/tex; mode=display">
			p(x_k | \mathcal{Y}_k) \approx \sum_{i = 1}^N w^{(i)}_{k} K(x_k - x^{(i)}_k).
		</script>
	</div>
	<p class="fragment">
		$K(x)$ is an appropriate kernel pdf that must satisfy certain conditions.
		Its type and hyperparameters have to be tailored to the concrete filtering problem.
	</p>
</section>

<section>
	<h3>Comparison to the Kalman filter family</h3>
	<ul>
		<li class="fragment"><p>EKF/UKF/EnKF: Compute the exact solution of a simpler, approximated problem;</p></li>
		<li class="fragment"><p>UKF: Deterministic sampling, statistical linearisation to a normal pdf;</p></li>
		<li class="fragment"><p>EnKF: Random sampling, statistical linearisation to a normal pdf;</p></li>
		<li class="fragment"><p>Mixture KF: Allows handling of multimodal pdfs.</p></li>
	</ul><br>
	<ul>
		<li class="fragment"><p>Particle filter: Compute the approximate solution of the exact problem.</p></li>
	</ul>

	<p class="fragment">
		Particle filtering is much more diverse than Kalman filtering.
		One also has more and more complex design choices:
	</p>
	<ul>
		<li class="fragment"><p>proposal pdf;</p></li>
		<li class="fragment"><p>resampling method;</p></li>
		<li class="fragment"><p>kernel type and its hyperparameters (RPF);</p></li>
		<li class="fragment"><p>process and measurement noise parameters, number of particles.</p></li>
	</ul>
</section>

<section>
	<h3>An example: Robot localisation (1/2)</h3>

	<div style="text-align:center;border-width: 0px">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/87htMQ6bX-g?rel=0" allowfullscreen></iframe>
	</div>

	<p class="fragment">
		Looks nice, but nothing really special.
		We could probably make an EKF with the same performance and much better runtime.
	</p>
</section>


<section>
	<h3>An example: Robot localisation (2/2)</h3>
	<div style="text-align:center;border-width: 0px">
		<iframe width="560" height="315" src="https://www.youtube.com/embed/vNGQFoCIsDg?rel=0" allowfullscreen></iframe>
	</div>

	<p class="fragment">
		Now you see the strength of the particle filtering:
		It faithfully approximates the posterior pdf as a multimodal density instead of being
		restricted to a unimodal Gaussian pdf.
	</p>
</section>


<section>
	<h3>Conclusion</h3>
	<p>
		Particle filter provide a <em>belief</em> about the current system state,
		i.e. a probability density represented by a finite number of weighted samples.
	</p>
	<p class="fragment">
		Particle filters can handle arbitrary nonlinearities and noise distributions,
		varying number of measurements or even missing measurements.
	</p>
	<p class="fragment">
		On the other hand, they can be very fickle and have to be tuned carefully.
	</p>
	<p class="fragment">
		A SIR PF is relatively easy to implement, but the runtime can be an issue.
	</p>
</section>

<section>
	<h3>References</h3>
	<p style="font-size: 75%">
		[1] M. Sanjeev Arulampalam, Simon Maskell, Neil Gordon and Tim Clapp.
		A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking.
		IEEE Transactions on Signal Processing, 50(2): 174–188, 2002.<br>
		DOI: <a href="http://dx.doi.org/10.1109/78.978374">10.1109/78.978374</a>.
	</p>
	<p style="font-size: 75%">
		[2] Christopher M. Bishop.
		Pattern Recognition and Machine Learning.
		Springer-Verlag New York Inc., 2006.
		ISBN 0387310738.
	</p>
	<p style="font-size: 75%">
		[3] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective.
		MIT Press, 2012.
		ISBN: 9780262018029.
	</p>
	<p style="font-size: 75%">
		[4] Sebastian Thrun, Wolfram Burgard and Dieter Fox.
		Probabilistic Robotics.
		MIT Press, 2005.<br>
		ISBN: 9780262201629.
	</p>
	<p style="font-size: 75%">
		[5] Roger R. Labbe Jr.
		Kalman and Bayesian Filters in Python.
		<a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python">Repository</a>,
		<a href="http://mybinder.org/repo/rlabbe/Kalman-and-Bayesian-Filters-in-Python">Binder</a>.
	</p>
	<p style="font-size: 75%">
		[6] Mikhail Pak.
		Two derivations of the Kalman filter.
		<a href="https://github.com/mp4096/two-derivations-of-the-kalman-filter">Repository</a>,
		<a href="https://mp4096.github.io/two-derivations-of-the-kalman-filter">Slides</a>.
	</p>
</section>


</div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
	Reveal.initialize({
		controls: false,
		progress: true,
		history: true,
		center: false,

		transition: 'fade',

		math: {
			// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
			config: 'TeX-AMS_HTML-full',
		},

		dependencies: [
			{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
			{ src: 'plugin/math/math.js', async: true },
			{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.configure({ classPrefix: '' }); hljs.initHighlightingOnLoad(); } },
		],
	});
	Reveal.configure({
		slideNumber: true,
		slideNumber: 'c/t',
	});
</script>
</body>
</html>
