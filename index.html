<!doctype html>
<html lang="en">

<!--
Copyright (C) 2016 Mikhail Pak
CC-BY-NC-SA 4.0

This slide deck uses reveal.js by Hakim El Hattab.
Copyright (C) 2016 Hakim El Hattab, http://hakim.se
MIT License
-->

<head>
	<meta charset="utf-8">

	<link href="https://fonts.googleapis.com/css?family=Open+Sans:200%7CRaleway:200%7CSource+Code+Pro" rel="stylesheet">

	<title>From the Kalman to the particle filter</title>

	<meta name="description" content="From the Kalman to the particle filter">
	<meta name="author" content="Mikhail Pak, TUM">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/winter.css" id="theme">

	<link rel="stylesheet" href="lib/css/color-brewer.css">
</head>

<body>
<div class="reveal">
<div class="slides">

<section class="center">
	<h2>From the Kalman to the particle filter</h2>
	<p style="font-size: 75%"><a style="color:#222" href="https://github.com/mp4096">Mikhail Pak</a>, November 2016</p>
	<p style="color:#EEE; font-size: 75%">CC-BY-NC-SA 4.0</p>
</section>

<section>
	<h3>Outline</h3>
	<ul>
		<li><p><a style="color:#222" href="#/2">Introduction</a></p></li>
		<li><p><a style="color:#222" href="#/5">Kalman filter as a Bayesian estimator</a></p></li>
		<li><p><a style="color:#222" href="#/6">Sequential importance sampling</a></p></li>
		<li><p><a style="color:#222" href="#/10">Making it work: SIR and RPF</a></p></li>
		<li><p><a style="color:#222" href="#/13">Comparison to the Kalman filter family</a></p></li>
		<li><p><a style="color:#222" href="#/14">An example: Face tracking</a></p></li>
		<li><p><a style="color:#222" href="#/15">Conclusion</a></p></li>
	</ul>
</section>

<section>
	<h3>Introduction 1/3</h3>
	<p>
		The Kalman filter is well-known in the control engineering community.
		Its extensions (EKF, UKF, EnKF, mixture KF) allow handling of nonlinear, non-Gaussian problems.
	</p>
	<div class="fragment">
		<p>On the other hand, we can also tackle the nonlinear filtering problem using a <em>particle filter</em>.
		In this talk, I will</p>
		<ul>
			<li class="fragment"><p>show how both the Kalman and the particle filter can be derived
				using the same Bayesian framework;</p></li>
			<li class="fragment"><p>present the most widespread variants of the particle filter;</p></li>
			<li class="fragment"><p>discuss the design choices for a particle filter;</p></li>
			<li class="fragment"><p>compare it with the extensions of the Kalman filter;</p></li>
			<li class="fragment"><p>show a simple demo of a particle filter for face tracking.</p></li>
		</ul>
	</div>
</section>

<section>
	<h3>Introduction 2/3</h3>
	<p>
		We consider a discrete-time autonomous system.
		Furthermore, we assume that it satisfies the Markov property.
	</p>
	<div class="fragment">
		<p>For a time instant $k$, we denote</p>
		<ul>
			<li class="fragment"><p>state by $x_k \in \mathbb{R}^n$;</p></li>
			<li class="fragment"><p>state trajectory $x_0, \ldots, x_k$ by $\mathcal{X}_k$;</p></li>
			<li class="fragment"><p>measurement by $y_k \in \mathbb{R}^q$;</p></li>
			<li class="fragment"><p>sequence of measurements $y_0, \ldots, y_k$ by $\mathcal{Y}_k$;</p></li>
			<li class="fragment"><p>transition pdf by $p(x_k | x_{k - 1})$;</p></li>
			<li class="fragment"><p>measurement pdf by $p(y_k | x_k)$.</p></li>
		</ul>
	</div>
</section>

<section>
	<h3>Introduction 3/3</h3>
	<p>Now, let’s revise Bayesian inference before we discuss the filtering problem.</p>
	<p>All derivations will be based on Bayes’ theorem:</p>
	<script type="math/tex; mode=display">
		p(A \,|\,B) = \frac{p(B \,|\,A) \ p(A)}{p(B)}
	</script>
	<div class="fragment">
	or, informally:
	<script type="math/tex; mode=display">
		p(\text{State} \,|\,\text{Measurements} )
		=
		\frac{
			p(\text{Measurements} \,|\,\text{State} )\ p(\text{State})
		}{
			p(\text{Measurements})
		}.
	</script>
	</div>
	<div class="fragment">
	<p>Here,</p>
	<ul>
		<li class="fragment"><p>$p(\text{State}\,|\,\text{Measurements})$ is the <em>posterior</em>;</p></li>
		<li class="fragment"><p>$p(\text{Measurements}\,|\,\text{State})$ is the <em>likelihood</em>;</p></li>
		<li class="fragment"><p>$p(\text{State})$ is the <em>prior</em>;</p></li>
		<li class="fragment"><p>$p(\text{Measurements})$ is the <em>total likelihood</em>.</p></li>
	</ul>
	</div>
</section>

<section>
	<h3>Kalman filter as a Bayesian estimator</h3>
	<p>
		We can derive the Kalman filter directly by plugging Gaussian pdfs into the Bayes formula:
	</p>
	<div class="fragment">
		<script type="math/tex; mode=display">
			p(x_k \,|\,y_k, \mathcal{Y}_{k - 1})
			=
			\frac{%
				p(y_k \,|\,x_k, \mathcal{Y}_{k - 1})
				\ %
				p(x_k \,|\,\mathcal{Y}_{k - 1})
			}{%
				p(y_k \,|\,\mathcal{Y}_{k - 1})
			}.
		</script>
	</div>
	<div style="text-align:center" class="fragment">
		<p>$\downarrow$</p>
		<script type="math/tex; mode=display">
			\left. \mathcal{N}(\hat{x}_k, \ P_k) \right|_{x_k}
			=
			\frac{%
				\left. \mathcal{N}(C_k x_k, \ R_k) \right|_{y_k}
				\ %
				\left. \mathcal{N}(\hat{x}_k^-, \ P_k^-) \right|_{x_k}
			}{%
				\left. \mathcal{N}(\hat{y}_k^-, \ C_k P_k^- C_k^\mathrm{T} + R_k) \right|_{y_k}
			}.
		</script><br>
	</div>
	<div class="fragment">
		<p>After some math, we obtain:</p>
		<script type="math/tex; mode=display">
			\begin{aligned}
				\hat{x}_k &= \hat{x}_k^- + P_k C_k^\mathrm{T} R_k^{-1} (y_k - \hat{y}_k^-), \\
				P_k  &= \left( (P_k^-)^{-1} + C_k^\mathrm{T} R_k^{-1} C_k \right)^{-1}.
				% = \left( I - P_k^- C_k^\mathrm{T} (C_k P_k^- C_k^\mathrm{T} + R_k)^{-1} C_k \right) P_k^-.
			\end{aligned}
		</script>
	</div>
</section>

<section>
	<h3>Sequential importance sampling 1/4</h3>
	<p>
		The main idea of a particle filter is to approximate the posterior pdf
		by a finite number of samples (particles):
	</p>
	<script type="math/tex; mode=display">
		p(\mathcal{X}_k | \mathcal{Y}_k) \approx \sum_{i = 1}^{N} \delta(\mathcal{X}_k - \mathcal{X}^{(i)}_k),
	</script>
	<p class="fragment">
		where $\delta$ is the Dirac measure and $\mathcal{X}^{(i)}_k$ is the $i$-th sample drawn from $p(\mathcal{X}_k | \mathcal{Y}_k)$.
	</p><br>
	<div class="fragment">
		<p>Unfortunately, this naïve approach has two significant disadvantages:</p>
		<ol>
			<li class="fragment"><p>
				it converges very slowly, since we sample from the whole state space,
				including its &lsquo;unimportant&rsquo; regions.
			</p></li>
			<li class="fragment"><p>
				the posterior pdf itself is unknown; furthermore, it can be very difficult to draw samples
				from this distribution.
			</p></li>
		</ol>
	</div>
</section>

<section>
	<h3>Sequential importance sampling 2/4</h3>
	<p>
		Enter <em>importance sampling</em>.
	</p>
	<p>
		We can extend a probability distribution $p(x)$ by
		$q(x) q(x)^{-1} = 1$ without changing it.
		Hence, we can approximate the posterior pdf
		by a finite number of <em>weighted</em> samples:
	</p>
	<script type="math/tex; mode=display">
		p(\mathcal{X}_k | \mathcal{Y}_k) \approx \sum_{i = 1}^{N} w^{(i)}_k \delta(\mathcal{X}_k - \mathcal{X}^{(i)}_k),
	</script>
	<div class="fragment">
		<p>
			where $\mathcal{X}^{(i)}_k$ are drawn from the <em>proposal pdf</em> $q(\mathcal{X}_k | \mathcal{Y}_k)$
			and the weights are chosen as
		</p>
		<script type="math/tex; mode=display">
			w^{(i)}_k \propto \frac{p(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)}{q(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)}.
		</script><br>
	</div>
	<div class="fragment">
		<p>
			Obviously, we would try to choose $q(\mathcal{X}_k | \mathcal{Y}_k)$ such that it is easy to sample.
			Furthermore, we restrict it to the following form:
		</p>
		<script type="math/tex; mode=display">
			q(\mathcal{X}_k | \mathcal{Y}_k) =
				q(x_k | \mathcal{X}_{k - 1}, \mathcal{Y}_k) \, q(\mathcal{X}_{k - 1} | \mathcal{Y}_{k - 1}).
		</script>
	</div>
</section>

<section>
	<h3>Sequential importance sampling 3/4</h3>
	<p>
		Now we have to derive the weight update equation. Apply Bayes' theorem:
	</p>
	<script type="math/tex; mode=display">
		\begin{aligned}
			p(\mathcal{X}_k | \mathcal{Y}_k) &=
			\frac{
				p(y_k | \mathcal{X}_k, \mathcal{Y}_{k - 1}) \, p(\mathcal{X}_k | \mathcal{Y}_{k - 1})
			}{p(y_k | \mathcal{Y}_{k - 1})} \\[0.5 em]
			&\propto
				p(y_k | x_k) \, p(x_k | x_{k - 1}) p(\mathcal{X}_{k - 1} | \mathcal{Y}_{k - 1})
		\end{aligned}
	</script>
	<div class="fragment">
		<p>and thus</p>
		<script type="math/tex; mode=display">
			\begin{aligned}
				w^{(i)}_k &\propto
				\frac{p(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)}{q(\mathcal{X}^{(i)}_k | \mathcal{Y}_k)} \\[0.5 em]
				&\propto
				\frac{
						p(y_k | x^{(i)}_k) \, p(x^{(i)}_k | x^{(i)}_{k - 1}) p(\mathcal{X}^{(i)}_{k - 1} | \mathcal{Y}_{k - 1})
				}{q(x^{(i)}_k | \mathcal{X}^{(i)}_{k - 1}, \mathcal{Y}_k) \, q(\mathcal{X}^{(i)}_{k - 1} | \mathcal{Y}_{k - 1})} \\[0.5 em]
 				&\propto
 				\frac{p(y_k | x^{(i)}_k) \, p(x^{(i)}_k | x^{(i)}_{k - 1})}{q(x^{(i)}_k | \mathcal{X}^{(i)}_{k - 1}, \mathcal{Y}_k)}
				w^{(i)}_{k - 1}.
			\end{aligned}
		</script>
	</div>
</section>

<section>
	<h3>Sequential importance sampling 4/4</h3>
	<p>
		We can also restrict the proposal density to $q(x_k | x_{k - 1}, y_k)$. Then,
	</p><br>
	<script type="math/tex; mode=display">
		w^{(i)}_k \propto
		\frac{p(y_k | x^{(i)}_k) \, p(x^{(i)}_k | x^{(i)}_{k - 1})}{q(x^{(i)}_k | x^{(i)}_{k - 1}, y_k)}
		w^{(i)}_{k - 1}
	</script><br>
	<div class="fragment">
		<p>and we can approximate the most recent state pdf as</p>
		<script type="math/tex; mode=display">
			p(x_k | \mathcal{Y}_k) \approx \sum_{i = 1}^{N} w^{(i)}_k \delta(x_k - x^{(i)}_k),
		</script><br>
	</div>
	<p class="fragment">
		We can summarise this algorithm as follows: 1) Draw samples from the proposal pdf;
		2) Update the weigths. This is the so called <em>sequential importance sampling (SIS)</em>.
	</p>
</section>

<section>
	<h3>Making it work: SIR and RPF 1/3</h3>
	<p>
		The plain SIS algorithm suffers from the <em>degeneracy phenomenon</em>, i.e. the variance
		of the weights increases over time. Furthermore, we still have to choose an appropriate
		proposal pdf $q(x_k | x_{k - 1}, y_k)$.
	</p>
	<p class="fragment">
		The <em>sequential importance resampling (SIR)</em> algorithm is the most widespread particle filtering algorithm.
		Its first main &lsquo;ingredient&rsquo; is to resample the particles after each step to avoid degeneracy.
		There are many resampling methods, e.g. multinomial, residual, systematic and stratified.
		Due to resampling, $w^{(i)}_{k - 1} = N^{-1}$ $\forall i$.
	</p>
	<p class="fragment">
		The second &lsquo;ingredient&rsquo; is to use the prior density as a proposal pdf<br><br>
		<script type="math/tex; mode=display">
			q(x_k | x^{(i)}_{k - 1}, y_k) = p(x_k | x^{(i)}_{k - 1})
		</script><br>
		which, together with resampling, leads us to a very simple weight update equation:<br><br>
		<script type="math/tex; mode=display">
			w^{(i)}_k \propto p(y_k | x^{(i)}_k).
		</script>
	</p>
</section>

<section>
	<h3>Making it work: SIR and RPF 2/3</h3>
	<p>
		Thus, we can finally specify an algorithm for a usable particle filter:
	</p>
<pre><code class="lang-python" data-trim data-noescape>
while True:
    y_curr = get_measurements()

    # Propagate particles and update their weights
    for i in range(N):
        x_curr[i] = sample_from_prior(x_prev)
        w_curr[i] = update_weights(y_curr, x_curr[i])

    # Normalise weights
    w_curr /= sum(w_curr)

    # Resample particles according to their weights
    x_prev = resample(x_curr, w_curr)
</code></pre><br>
</section>

<section>
	<h3>Making it work: SIR and RPF 3/3</h3>
	<p>
		Unfortunately, SIR itself suffers from <em>sample impoverishment</em>,
		especially if the process noise is small.
		In this case, a <em>regularised particle filter</em> might be a good solution.
	</p>
	<p class="fragment">
		A regularised particle filter is identical to the SIR except that it resamples from
		a continuous approximation for the posterior pdf:<br><br>
		<script type="math/tex; mode=display">
			p(x_k | \mathcal{Y}_k) \approx \sum_{i = 1}^N w^{(i)}_{k} K(x_k - x^{(i)}_k).
		</script>
	</p>
	<p class="fragment">
		$K(x)$ is an appropriate kernel pdf such that<br><br>
		<script type="math/tex; mode=display">
		\begin{aligned}
			&\int x \, K(x) dx = 0 &
			&\text{and} &
			&\int \|x\|^2 K(x) dx < \infty.
		\end{aligned}
		</script>
	</p>
	<p class="fragment">
		The kernel type and its hyperparameters are a design choice depending on
		the formulation of the particle filter.
	</p>
</section>

<section>
	<h3>Comparison to the Kalman filter family</h3>
	<ul>
		<li class="fragment"><p>EKF/UKF/EnKF: Compute the exact solution of a simpler, approximated problem;</p></li>
		<li class="fragment"><p>UKF: Deterministic sampling, statistical linearisation to a normal pdf;</p></li>
		<li class="fragment"><p>EnKF: Random sampling, statistical linearisation to a normal pdf;</p></li>
		<li class="fragment"><p>Mixture KF: Allows handling of multimodal pdfs.</p></li>
	</ul><br>
	<ul>
		<li class="fragment"><p>Particle filter: Compute the approximate solution of the exact problem.</p></li>
	</ul>

	<p class="fragment">
		Particle filtering is much more diverse than Kalman filtering.
		One also has more and more complex design choices:
	</p>
	<ul>
		<li class="fragment"><p>proposal pdf;</p></li>
		<li class="fragment"><p>resampling method;</p></li>
		<li class="fragment"><p>number of particles.</p></li>
	</ul>
</section>

<section>
	<h3>An example: Face tracking</h3>
</section>

<section>
	<h3>Conclusion</h3>
	<p>
	</p>
	<p>
	</p>
</section>

<section>
	<h3>References</h3>
	<p style="font-size: 75%">
		[1] M. Sanjeev Arulampalam, Simon Maskell, Neil Gordon and Tim Clapp.
		A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking.
		IEEE Transactions on Signal Processing, 50(2): 174–188, 2002.<br>
		DOI: <a href="http://dx.doi.org/10.1109/78.978374">10.1109/78.978374</a>.
	</p>
	<p style="font-size: 75%">
		[2] Christopher M. Bishop.
		Pattern Recognition and Machine Learning.
		Springer-Verlag New York Inc., 2006.
		ISBN 0387310738.
	</p>
	<p style="font-size: 75%">
		[3] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective.
		MIT Press, 2012.
		ISBN: 9780262018029.
	</p>
	<p style="font-size: 75%">
		[4] Sebastian Thrun, Wolfram Burgard and Dieter Fox.
		Probabilistic Robotics.
		MIT Press, 2005.<br>
		ISBN: 9780262201629.
	</p>
	<p style="font-size: 75%">
		[4] Roger R. Labbe Jr.
		Kalman and Bayesian Filters in Python.
		<a href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python">Repository</a>,
		<a href="http://mybinder.org/repo/rlabbe/Kalman-and-Bayesian-Filters-in-Python">Binder</a>.
	</p>
	<p style="font-size: 75%">
		[4] Mikhail Pak.
		Two derivations of the Kalman filter.
		<a href="https://github.com/mp4096/two-derivations-of-the-kalman-filter">Repository</a>,
		<a href="https://mp4096.github.io/two-derivations-of-the-kalman-filter">Slides</a>.
	</p>
</section>


</div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
	Reveal.initialize({
		controls: false,
		progress: true,
		history: true,
		center: false,

		transition: 'fade',

		math: {
			// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
			config: 'TeX-AMS_HTML-full',
		},

		dependencies: [
			{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
			{ src: 'plugin/math/math.js', async: true },
			{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.configure({ classPrefix: '' }); hljs.initHighlightingOnLoad(); } },
		],
	});
	Reveal.configure({
		slideNumber: true,
		slideNumber: 'c/t',
	});
</script>
</body>
</html>
